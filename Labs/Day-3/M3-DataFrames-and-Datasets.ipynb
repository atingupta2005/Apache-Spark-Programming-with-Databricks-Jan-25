{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9828c9d",
   "metadata": {},
   "source": [
    "**Module 3: DataFrames and Datasets**\n",
    "\n",
    "### Understanding DataFrames and Datasets\n",
    "\n",
    "**1. What is a Schema?**\n",
    "A schema defines the structure of a dataset, specifying the column names, data types, and constraints. It ensures that the data adheres to a predefined format, enabling consistency and compatibility across processing workflows.\n",
    "\n",
    "- **Benefits of Schema:**\n",
    "  - Enforces data integrity and validation.\n",
    "  - Enables query optimization.\n",
    "  - Facilitates compatibility with downstream applications.\n",
    "\n",
    "*Example*: Viewing the schema of a DataFrame:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125430cb",
   "metadata": {},
   "source": [
    "**2. DataFrames**\n",
    "A DataFrame is a distributed collection of data organized into named columns, similar to a table in a relational database. It is designed to support both structured and semi-structured data processing.\n",
    "\n",
    "- **Key Features of DataFrames:**\n",
    "  - Schema-based processing.\n",
    "  - Lazy evaluation for efficient execution.\n",
    "  - Catalyst optimizer for query planning.\n",
    "  - Multi-language support: Python, Scala, SQL, R.\n",
    "\n",
    "*Example*: Loading a JSON dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a3fc90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load JSON data\n",
    "flight_data_json = \"dbfs:/mnt/data/data/flight-data/json/2015-summary.json\"\n",
    "df = spark.read.format(\"json\").load(flight_data_json)\n",
    "\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897e2c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da646b2c",
   "metadata": {},
   "source": [
    "**3. Datasets**\n",
    "Datasets provide type-safe operations with compile-time type checking, combining the benefits of RDDs with the optimizations of DataFrames. While primarily used in Scala and Java, the equivalent in Python is the DataFrame API.\n",
    "\n",
    "- **Differences Between DataFrames and Datasets:**\n",
    "  - **Type Safety**: Datasets offer type safety, while DataFrames do not.\n",
    "  - **Languages**: Datasets are native to Scala and Java; DataFrames are universal.\n",
    "  - **Serialization**: Datasets use encoders for object serialization.\n",
    "\n",
    "---\n",
    "\n",
    "### File Formats in Spark\n",
    "\n",
    "**1. JSON (JavaScript Object Notation)**\n",
    "A lightweight, semi-structured format ideal for data exchange.\n",
    "\n",
    "- **Advantages:**\n",
    "  - Human-readable.\n",
    "  - Flexible structure for nested data.\n",
    "\n",
    "*Example*: Reading and writing JSON:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0d13b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading JSON\n",
    "json_df = spark.read.format(\"json\").load(\"dbfs:/mnt/data/data/flight-data/json/2015-summary.json\")\n",
    "\n",
    "# Writing JSON\n",
    "json_df.write.format(\"json\").save(\"dbfs:/mnt/data/output/flight-data-json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b303b53e",
   "metadata": {},
   "source": [
    "**2. Parquet**\n",
    "A columnar storage format optimized for analytics, offering efficient compression and query performance.\n",
    "\n",
    "- **Advantages:**\n",
    "  - Supports schema evolution.\n",
    "  - Optimized for columnar operations.\n",
    "\n",
    "*Example*: Working with Parquet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d433204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading Parquet\n",
    "parquet_df = spark.read.format(\"parquet\").load(\"dbfs:/mnt/data/data/flight-data/parquet/2010-summary.parquet\")\n",
    "\n",
    "# Writing Parquet\n",
    "parquet_df.write.format(\"parquet\").save(\"dbfs:/mnt/data/output/flight-data-parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54c0f03",
   "metadata": {},
   "source": [
    "**3. Avro**\n",
    "A row-based binary format commonly used for data serialization.\n",
    "\n",
    "- **Advantages:**\n",
    "  - Compact and efficient.\n",
    "  - Supports schema definition and evolution.\n",
    "\n",
    "*Example*: Working with Avro:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7cf2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading Avro\n",
    "avro_df = spark.read.format(\"avro\").load(\"dbfs:/mnt/data/data/flight-data/avro/2010-summary.avro\")\n",
    "\n",
    "# Writing Avro\n",
    "avro_df.write.format(\"avro\").save(\"dbfs:/mnt/data/output/flight-data-avro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159c1304",
   "metadata": {},
   "source": [
    "**4. CSV (Comma-Separated Values)**\n",
    "A widely used format for simple tabular data.\n",
    "\n",
    "- **Advantages:**\n",
    "  - Easy to understand and use.\n",
    "  - Compatible with many tools.\n",
    "\n",
    "*Example*: Working with CSV:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3ff411",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading CSV\n",
    "csv_df = spark.read.format(\"csv\").option(\"header\", True).load(\"dbfs:/mnt/data/data/flight-data/csv/2010-summary.csv\")\n",
    "\n",
    "# Writing CSV\n",
    "csv_df.write.format(\"csv\").option(\"header\", True).save(\"dbfs:/mnt/data/output/flight-data-csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bafc35b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Schema Management and Optimization\n",
    "\n",
    "**1. Inferring Schema**\n",
    "Spark automatically detects column names and data types based on file contents.\n",
    "\n",
    "*Example*: Inferring schema from JSON:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c29613b",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_inferred_df = spark.read.json(flight_data_json)\n",
    "schema_inferred_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5d8ded",
   "metadata": {},
   "source": [
    "**2. Explicit Schema Definition**\n",
    "Define schemas for strict data validation and compatibility.\n",
    "\n",
    "*Example*: Defining a schema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d769ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, LongType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"DEST_COUNTRY_NAME\", StringType(), True),\n",
    "    StructField(\"ORIGIN_COUNTRY_NAME\", StringType(), True),\n",
    "    StructField(\"count\", LongType(), True)\n",
    "])\n",
    "\n",
    "schema_defined_df = spark.read.format(\"json\").schema(schema).load(flight_data_json)\n",
    "schema_defined_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766cf436",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Practical Use Cases\n",
    "\n",
    "**1. Joins**\n",
    "Combine datasets to analyze relationships.\n",
    "\n",
    "*Example*: Inner join:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19588403",
   "metadata": {},
   "outputs": [],
   "source": [
    "flights_2010 = spark.read.json(\"dbfs:/mnt/data/data/flight-data/json/2010-summary.json\")\n",
    "joined_df = df.join(flights_2010, df.DEST_COUNTRY_NAME == flights_2010.DEST_COUNTRY_NAME, \"inner\")\n",
    "joined_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63aa8c11",
   "metadata": {},
   "source": [
    "**2. Aggregations**\n",
    "Summarize data using group operations.\n",
    "\n",
    "*Example*: Calculating totals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63dab93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum\n",
    "agg_df = df.groupBy(\"DEST_COUNTRY_NAME\").agg(sum(\"count\").alias(\"total_count\"))\n",
    "agg_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5317e0",
   "metadata": {},
   "source": [
    "**3. Filters**\n",
    "Select specific subsets of data.\n",
    "\n",
    "*Example*: Filtering data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee8889c",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = df.filter(df[\"count\"] > 1000)\n",
    "filtered_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041639f7",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
