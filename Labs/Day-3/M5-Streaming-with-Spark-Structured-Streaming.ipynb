{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d0b0c66",
   "metadata": {},
   "source": [
    "**Module 5: Streaming with Spark Structured Streaming**\n",
    "\n",
    "### Real-Time Data Processing Concepts\n",
    "\n",
    "Structured Streaming is a scalable and fault-tolerant stream processing engine built on the Spark SQL engine. It enables real-time processing of data streams using the same abstractions as batch processing, making it easy to build scalable data pipelines.\n",
    "\n",
    "**Key Features:**\n",
    "- **Incremental Query Execution**: Continuously processes streaming data as new records arrive.\n",
    "- **Fault Tolerance**: Uses checkpoints and write-ahead logs for resilience.\n",
    "- **Exactly-Once Processing**: Ensures data consistency across streaming and batch workloads.\n",
    "- **Unification with Spark SQL and DataFrames**: Uses the same DataFrame/Dataset API for batch and streaming workloads.\n",
    "\n",
    "#### **Example 1: Setting Up a Streaming DataFrame**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb36be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Create a Spark Session\n",
    "spark = SparkSession.builder.appName(\"StructuredStreamingExample\").getOrCreate()\n",
    "\n",
    "# Read a stream from a directory\n",
    "df = spark.readStream.format(\"json\").option(\"path\", \"dbfs:/mnt/data/data/streaming-json/\").load()\n",
    "\n",
    "# Inspect the schema of the streaming DataFrame\n",
    "df.printSchema()\n",
    "\n",
    "# Select specific columns\n",
    "df_selected = df.select(\"timestamp\", \"value\")\n",
    "\n",
    "# Apply filtering to exclude records with low values\n",
    "df_filtered = df_selected.filter(col(\"value\") > 10)\n",
    "\n",
    "# Add a new column with transformed data\n",
    "df_transformed = df_filtered.withColumn(\"normalized_value\", col(\"value\") / 100)\n",
    "\n",
    "# Write transformed stream to the console\n",
    "df_transformed.writeStream.format(\"console\").outputMode(\"append\").start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75372f3",
   "metadata": {},
   "source": [
    "#### **Example 2: Processing Streaming Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a598e6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select specific columns and filter data\n",
    "df_selected = df.select(\"timestamp\", \"value\").filter(col(\"value\") > 10)\n",
    "\n",
    "# Group by and aggregate the stream\n",
    "df_grouped = df_selected.groupBy(\"timestamp\").agg({\"value\": \"sum\"})\n",
    "\n",
    "# Write aggregated data to memory query\n",
    "query = df_grouped.writeStream.format(\"memory\").queryName(\"streaming_aggregates\").outputMode(\"complete\").start()\n",
    "\n",
    "# Query the results in memory\n",
    "spark.sql(\"SELECT * FROM streaming_aggregates ORDER BY timestamp DESC\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8fcc18",
   "metadata": {},
   "source": [
    "#### **Example 3: Writing Stream to Memory with Additional Transformations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7819834",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import year, month, dayofmonth\n",
    "\n",
    "# Extract date components from the timestamp\n",
    "df_with_date = df.withColumn(\"year\", year(\"timestamp\"))\\\n",
    "                  .withColumn(\"month\", month(\"timestamp\"))\\\n",
    "                  .withColumn(\"day\", dayofmonth(\"timestamp\"))\n",
    "\n",
    "# Filter for records in a specific year\n",
    "df_filtered = df_with_date.filter(col(\"year\") == 2025)\n",
    "\n",
    "# Write filtered results to memory\n",
    "query = df_filtered.writeStream\\\n",
    "    .format(\"memory\")\\\n",
    "    .queryName(\"filtered_stream\")\\\n",
    "    .outputMode(\"append\")\\\n",
    "    .start()\n",
    "\n",
    "# Query the memory table\n",
    "spark.sql(\"SELECT * FROM filtered_stream\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ace8d7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Sources: Kafka, Delta Lake, and Files\n",
    "\n",
    "#### **1. Streaming from Kafka**\n",
    "Kafka is a popular distributed messaging system used for high-throughput, low-latency data streams.\n",
    "\n",
    "#### **Example 4: Reading Data from Kafka and Applying Transformations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde63c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_df = spark.readStream.format(\"kafka\")\\\n",
    "    .option(\"kafka.bootstrap.servers\", \"<BROKER_URL>\")\\\n",
    "    .option(\"subscribe\", \"topic_name\")\\\n",
    "    .load()\n",
    "\n",
    "# Deserialize Kafka messages\n",
    "deserialized_df = kafka_df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")\n",
    "\n",
    "# Add processing timestamp and filter messages\n",
    "deserialized_df = deserialized_df.withColumn(\"processed_at\", current_timestamp())\\\n",
    "                                 .filter(col(\"value\").isNotNull())\n",
    "\n",
    "# Write transformed messages to the console\n",
    "deserialized_df.writeStream.format(\"console\").start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23c2df7",
   "metadata": {},
   "source": [
    "#### **Example 5: Writing Transformed Kafka Data Back to Kafka**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec427af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df = deserialized_df.selectExpr(\"CAST(key AS STRING) AS key\", \"CAST(value AS STRING) AS value\")\n",
    "output_df.writeStream.format(\"kafka\")\\\n",
    "    .option(\"kafka.bootstrap.servers\", \"<BROKER_URL>\")\\\n",
    "    .option(\"topic\", \"output_topic\")\\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a5f58b",
   "metadata": {},
   "source": [
    "#### **2. Streaming from Delta Lake with Transformations**\n",
    "\n",
    "#### **Example 6: Streaming Aggregated Data from Delta Lake**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301b37a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg, col\n",
    "\n",
    "delta_df = spark.readStream.format(\"delta\").option(\"path\", \"dbfs:/mnt/data/data/delta-table/\").load()\n",
    "\n",
    "# Perform aggregation on streaming data\n",
    "delta_aggregated = delta_df.groupBy(\"category\").agg(avg(col(\"value\")).alias(\"avg_value\"))\n",
    "\n",
    "# Write aggregated results to a new Delta table\n",
    "delta_aggregated.writeStream\\\n",
    "    .format(\"delta\")\\\n",
    "    .outputMode(\"complete\")\\\n",
    "    .option(\"checkpointLocation\", \"dbfs:/mnt/data/checkpoints/delta/\")\\\n",
    "    .option(\"path\", \"dbfs:/mnt/data/output/delta-aggregated/\")\\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01005841",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Window Functions and Watermarking\n",
    "\n",
    "#### **Example 7: Complex Tumbling Window Aggregation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9adfc5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import window\n",
    "\n",
    "# Perform window-based aggregation on streaming data\n",
    "aggregated_df = df.groupBy(window(col(\"timestamp\"), \"10 minutes\"), \"category\").count()\n",
    "\n",
    "# Write results to memory\n",
    "tag_query = aggregated_df.writeStream\\\n",
    "    .outputMode(\"update\")\\\n",
    "    .format(\"memory\")\\\n",
    "    .queryName(\"windowed_aggregates\")\\\n",
    "    .start()\n",
    "\n",
    "# Query the results in memory\n",
    "spark.sql(\"SELECT * FROM windowed_aggregates\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb36f079",
   "metadata": {},
   "source": [
    "#### **Example 8: Using Watermarking for Late Data Management**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9b562d",
   "metadata": {},
   "outputs": [],
   "source": [
    "watermarked_df = df.withWatermark(\"timestamp\", \"5 minutes\")\\\n",
    "    .groupBy(window(col(\"timestamp\"), \"10 minutes\"), \"category\").count()\n",
    "\n",
    "# Write watermarked results to the console\n",
    "watermarked_df.writeStream.format(\"console\").outputMode(\"update\").start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de14d5c3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Fault Tolerance and Checkpointing\n",
    "\n",
    "#### **Example 9: Checkpointing with Multiple Sinks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2837b39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write stream data to Parquet and Delta sinks with checkpointing\n",
    "query_parquet = df.writeStream\\\n",
    "    .format(\"parquet\")\\\n",
    "    .option(\"checkpointLocation\", \"dbfs:/mnt/data/checkpoints/parquet/\")\\\n",
    "    .option(\"path\", \"dbfs:/mnt/data/output/parquet/\")\\\n",
    "    .start()\n",
    "\n",
    "query_delta = df.writeStream\\\n",
    "    .format(\"delta\")\\\n",
    "    .option(\"checkpointLocation\", \"dbfs:/mnt/data/checkpoints/delta/\")\\\n",
    "    .option(\"path\", \"dbfs:/mnt/data/output/delta/\")\\\n",
    "    .start()\n",
    "\n",
    "query_parquet.awaitTermination()\n",
    "query_delta.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a403050",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
