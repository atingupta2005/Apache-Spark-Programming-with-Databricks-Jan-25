{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "home = \"dbfs:/mnt/data\"\n",
    "\n",
    "path_train_data = f\"{home}/data/Automobile-Loan-Default/Train_Dataset.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructField, StructType, StringType, LongType, DoubleType, DoubleType\n",
    "my_schema = StructType([StructField(\"ID\",StringType(),True),StructField(\"Client_Income\",DoubleType(),True),\\\n",
    "                        StructField(\"Car_Owned\",DoubleType(),True),StructField(\"Bike_Owned\",DoubleType(),True),\\\n",
    "                        StructField(\"Active_Loan\",DoubleType(),True),StructField(\"House_Own\",DoubleType(),True),\\\n",
    "                        StructField(\"Child_Count\",DoubleType(),True),StructField(\"Credit_Amount\",DoubleType(),True),\\\n",
    "                        StructField(\"Loan_Annuity\",DoubleType(),True),StructField(\"Accompany_Client\",StringType(),True),\\\n",
    "                        StructField(\"Client_Income_Type\",StringType(),True),StructField(\"Client_Education\",StringType(),True),\\\n",
    "                        StructField(\"Client_Marital_Status\",StringType(),True),StructField(\"Client_Gender\",StringType(),True),\\\n",
    "                        StructField(\"Loan_Contract_Type\",StringType(),True),StructField(\"Client_Housing_Type\",StringType(),True),\\\n",
    "                        StructField(\"Population_Region_Relative\",DoubleType(),True),StructField(\"Age_Days\",DoubleType(),True),\\\n",
    "                        StructField(\"Employed_Days\",DoubleType(),True),StructField(\"Registration_Days\",DoubleType(),True),\\\n",
    "                        StructField(\"ID_Days\",DoubleType(),True),StructField(\"Own_House_Age\",DoubleType(),True),\\\n",
    "                        StructField(\"Mobile_Tag\",DoubleType(),True),StructField(\"Homephone_Tag\",DoubleType(),True),\\\n",
    "                        StructField(\"Workphone_Working\",DoubleType(),True),StructField(\"Client_Occupation\",StringType(),True),\\\n",
    "                        StructField(\"Client_Family_Members\",DoubleType(),True),StructField(\"Cleint_City_Rating\",DoubleType(),True),\\\n",
    "                        StructField(\"Application_Process_Day\",DoubleType(),True),StructField(\"Application_Process_Hour\",DoubleType(),True),\\\n",
    "                        StructField(\"Client_Permanent_Match_Tag\",StringType(),True),StructField(\"Client_Contact_Work_Tag\",StringType(),True),\\\n",
    "                        StructField(\"Type_Organization\",StringType(),True),StructField(\"Score_Source_1\",DoubleType(),True),\\\n",
    "                        StructField(\"Score_Source_2\",DoubleType(),True),StructField(\"Score_Source_3\",DoubleType(),True),\\\n",
    "                        StructField(\"Social_Circle_Default\",DoubleType(),True),StructField(\"Phone_Change\",DoubleType(),True),\\\n",
    "                        StructField(\"Credit_Bureau\",DoubleType(),True),StructField(\"Default\",DoubleType(),True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = ['Score_Source_2', 'Employed_Days', 'Age_Days', 'Client_Education', \\\n",
    "                     'Client_Gender', 'ID_Days', 'Population_Region_Relative', 'Credit_Amount', \\\n",
    "                     'Car_Owned', 'Child_Count', 'Loan_Annuity']\n",
    "\n",
    "selected_features = ['Client_Education', 'Employed_Days', 'Age_Days', 'Client_Income_Type', \\\n",
    " 'Client_Gender', 'Car_Owned', 'ID_Days', 'Score_Source_2', 'Phone_Change']\n",
    "\n",
    "target_variable_name = \"Default\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_data = spark\\\n",
    ".read\\\n",
    ".schema(my_schema)\\\n",
    ".option(\"header\", \"true\")\\\n",
    ".csv(path_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_get = selected_features + [target_variable_name]\n",
    "df_train_data = df_train_data.select(*cols_to_get)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Length of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "121856"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Describe data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>summary</th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>stddev</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Client_Education</th>\n",
       "      <td>118194</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Graduation</td>\n",
       "      <td>Secondary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Employed_Days</th>\n",
       "      <td>118190</td>\n",
       "      <td>67154.07061511127</td>\n",
       "      <td>138971.78295053402</td>\n",
       "      <td>0.0</td>\n",
       "      <td>365243.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Age_Days</th>\n",
       "      <td>118239</td>\n",
       "      <td>16027.422948434949</td>\n",
       "      <td>4366.356503618858</td>\n",
       "      <td>7676.0</td>\n",
       "      <td>25201.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Client_Income_Type</th>\n",
       "      <td>118139</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Businessman</td>\n",
       "      <td>Unemployed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Client_Gender</th>\n",
       "      <td>119426</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Female</td>\n",
       "      <td>XNA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Car_Owned</th>\n",
       "      <td>118258</td>\n",
       "      <td>0.34287743746723265</td>\n",
       "      <td>0.4746729459548691</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID_Days</th>\n",
       "      <td>115871</td>\n",
       "      <td>2987.471015180675</td>\n",
       "      <td>1511.8845759418805</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7197.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Score_Source_2</th>\n",
       "      <td>116154</td>\n",
       "      <td>0.5186100569859782</td>\n",
       "      <td>0.7402967506270233</td>\n",
       "      <td>5.0E-6</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Phone_Change</th>\n",
       "      <td>118175</td>\n",
       "      <td>962.0740511952613</td>\n",
       "      <td>827.9477872022571</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4185.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Default</th>\n",
       "      <td>121839</td>\n",
       "      <td>0.08079514769490885</td>\n",
       "      <td>0.27252137780653685</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "summary              count                 mean               stddev  \\\n",
       "Client_Education    118194                 None                 None   \n",
       "Employed_Days       118190    67154.07061511127   138971.78295053402   \n",
       "Age_Days            118239   16027.422948434949    4366.356503618858   \n",
       "Client_Income_Type  118139                 None                 None   \n",
       "Client_Gender       119426                 None                 None   \n",
       "Car_Owned           118258  0.34287743746723265   0.4746729459548691   \n",
       "ID_Days             115871    2987.471015180675   1511.8845759418805   \n",
       "Score_Source_2      116154   0.5186100569859782   0.7402967506270233   \n",
       "Phone_Change        118175    962.0740511952613    827.9477872022571   \n",
       "Default             121839  0.08079514769490885  0.27252137780653685   \n",
       "\n",
       "summary                     min         max  \n",
       "Client_Education     Graduation   Secondary  \n",
       "Employed_Days               0.0    365243.0  \n",
       "Age_Days                 7676.0     25201.0  \n",
       "Client_Income_Type  Businessman  Unemployed  \n",
       "Client_Gender            Female         XNA  \n",
       "Car_Owned                   0.0         1.0  \n",
       "ID_Days                     0.0      7197.0  \n",
       "Score_Source_2           5.0E-6       100.0  \n",
       "Phone_Change                0.0      4185.0  \n",
       "Default                     0.0         1.0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_data.describe().toPandas().set_index(\"summary\").T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main concepts in Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- DataFrame:\n",
    "    - This ML API uses DataFrame from Spark SQL as an ML dataset, which can hold a variety of data types. E.g., a DataFrame could have different columns storing text, feature vectors, true labels, and predictions.\n",
    "- Transformer:\n",
    "    - A Transformer is an algorithm which can transform one DataFrame into another DataFrame. E.g., an ML model is a Transformer which transforms a DataFrame with features into a DataFrame with predictions.\n",
    "- Estimator:\n",
    "    - An Estimator is an algorithm which can be fit on a DataFrame to produce a Transformer. E.g., a learning algorithm is an Estimator which trains on a DataFrame and produces a model.\n",
    "- Pipeline:\n",
    "    - A Pipeline chains multiple Transformers and Estimators together to specify an ML workflow.\n",
    "- Parameter:\n",
    "    - A Param is a named parameter\n",
    "    - A ParamMap is a set of (parameter, value) pairs.\n",
    "    - There are two main ways to pass parameters to an algorithm:\n",
    "        - Set parameters for an instance. E.g., if lr is an instance of LogisticRegression, one could call lr.setMaxIter(10) to make lr.fit() use at most 10 iterations. This API resembles the API used in spark.mllib package.\n",
    "        - Pass a ParamMap to fit() or transform(). Any parameters in the ParamMap will override parameters previously specified via setter methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML persistence: Saving and Loading Pipelines\n",
    "- Often times it is worth it to save a model or a pipeline to disk for later use.\n",
    "- ML persistence works across Scala, Java and Python.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Binarizer, StringIndexer, OneHotEncoderEstimator, StandardScaler\n",
    "from pyspark.ml.feature import MinMaxScaler, VectorAssembler, Imputer\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions used to detect variable type and rename columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def variable_type(df):\n",
    "    \n",
    "    vars_list = df.dtypes\n",
    "    char_vars = []\n",
    "    num_vars = []\n",
    "    for i in vars_list:\n",
    "        if i[1] in ('string'):\n",
    "            char_vars.append(i[0])\n",
    "        else:\n",
    "            num_vars.append(i[0])\n",
    "    \n",
    "    return char_vars, num_vars\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_list = df.columns\n",
    "features_list.remove(target_variable_name)\n",
    "char_vars, num_vars = variable_type(df_train_data)\n",
    "char_columns = df_train_data.select(*char_vars).columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure an ML pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "stringIndexers = [StringIndexer(inputCol=c, outputCol=c+\"_index\", handleInvalid=\"keep\") for c in char_columns]\n",
    "\n",
    "# Create list of columns generated from string indexer. This list will be passed as input columns to next indexer\n",
    "idx_out_cols = list([idx.getOutputCol() for idx in stringIndexers])\n",
    "\n",
    "oneHotEncoderEstimator = OneHotEncoderEstimator(inputCols=idx_out_cols,outputCols=[idx_out_col+\"_OHE\" for idx_out_col in idx_out_cols])\n",
    "\n",
    "# Create list of columns generated from oneHotEncoderEstimator. This list will be passed as input columns to next indexer\n",
    "ohe_out_cols = oneHotEncoderEstimator.getOutputCols()\n",
    "\n",
    "# We need create vector of all the columns. \n",
    "    #- The variable num_vars contains the name of columns which were already of numeric type\n",
    "    #- ohe_out_cols contans the name of those variables which are converted from string to numeric type using string indexer and one hot encoder\n",
    "cols_vectorAssemblers = num_vars + ohe_out_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binarization is the process of thresholding numerical features to binary (0/1) features.\n",
    "binarizer = Binarizer(threshold=30, inputCol='Age_Days', outputCol='Age_Days_binarized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of columns which needs to be imputed. Created separate list to impute using different strategy\n",
    "num_vars_medean = ['Car_Owned', 'Bike_Owned', 'Active_Loan', 'House_Own', 'Child_Count', 'Mobile_Tag',\\\n",
    "                 'Homephone_Tag', 'Workphone_Working', 'Client_Family_Members', 'Cleint_City_Rating', \\\n",
    "                 'Credit_Bureau','Application_Process_Day','Default']\n",
    "\n",
    "num_vars_medean = list(set(df.columns).intersection(set(num_vars_medean)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The remaining colums will be imputed using mean strategy\n",
    "num_vars_mean = list(set(num_vars) - (set(num_vars_medean)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ID_Days', 'Employed_Days', 'Score_Source_2', 'Age_Days', 'Phone_Change']\n"
     ]
    }
   ],
   "source": [
    "print(num_vars_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer_mean = Imputer(inputCols=num_vars_mean, outputCols=num_vars_mean).setStrategy(\"mean\")\n",
    "imputer_mod = Imputer(inputCols=num_vars_medean, outputCols=num_vars_medean).setStrategy(\"median\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to scale the columns which have large values\n",
    "cols_to_scale = ['Social_Circle_Default','Phone_Change','Own_House_Age','Score_Source_3',\\\n",
    "                 'Application_Process_Hour','Population_Region_Relative','Credit_Amount','Client_Income',\\\n",
    "                 'Employed_Days','Loan_Annuity','Age_Days','ID_Days','Score_Source_2','Registration_Days','Score_Source_1']\n",
    "\n",
    "# Interecting as we might have filtered out some columns in the beginning due to feature selection approch done in last notebook - 10-a\n",
    "cols_to_scale = list(set(cols_to_scale).intersection(set(df.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Any one of below scalars need to be used.\n",
    "standardScaler = StandardScaler(inputCol='features', outputCol=\"features_scaled\")\n",
    "minMaxScaler = MinMaxScaler(inputCol='features', outputCol=\"features_scaled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VectorAssembler is the transformer that combines a given list of columns into a single vector column.\n",
    "vectorAssembler = VectorAssembler(inputCols=cols_vectorAssemblers, outputCol='features', handleInvalid = \"keep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create pipeline which have all the stages which we configured above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "stages = stringIndexers + [oneHotEncoderEstimator, imputer_mean,imputer_mod]  + [vectorAssembler, minMaxScaler,\\\n",
    "                                                           binarizer\n",
    "                                                          ]\n",
    "pipeline = Pipeline(stages=stages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[StringIndexer_e7df86393e40, StringIndexer_5a4254ab0bb0, StringIndexer_489b8f7a3b0a, OneHotEncoderEstimator_cc8a1d602a2b, Imputer_1c16f29b54b3, Imputer_50c35a140b14, VectorAssembler_c16f1d3884bc, MinMaxScaler_cad3f78cf8f3, Binarizer_40d0dd5a4bbf]\n"
     ]
    }
   ],
   "source": [
    "print(stages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit Data Processing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_train_data.limit(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Client_Education: string, Employed_Days: double, Age_Days: double, Client_Income_Type: string, Client_Gender: string, Car_Owned: double, ID_Days: double, Score_Source_2: double, Phone_Change: double, Default: double]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "def read_yaml(file_path):\n",
    "    with open(file_path, \"r\") as f:\n",
    "        return yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_config = read_yaml(\"configs/config.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PIPELINE': {'FIT': {'imputer_strategy': 'mean',\n",
       "   'standardScaler_withStd': True,\n",
       "   'standardScaler_withMean': True},\n",
       "  'TRANSFORM': {'imputer_strategy': 'mean',\n",
       "   'standardScaler_withStd': True,\n",
       "   'standardScaler_withMean': False}}}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_config['PIPELINE']['FIT']['standardScaler_withMean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{Param(parent='Binarizer_40d0dd5a4bbf', name='threshold', doc='threshold in binary classification prediction, in range [0, 1]'): 20000,\n",
       " Param(parent='Imputer_1c16f29b54b3', name='strategy', doc='strategy for imputation. If mean, then replace missing values using the mean value of the feature. If median, then replace missing values using the median value of the feature.'): 'mean',\n",
       " Param(parent='StandardScaler_2b12c1b1db3e', name='withStd', doc='Scale to unit standard deviation'): True,\n",
       " Param(parent='StandardScaler_2b12c1b1db3e', name='withMean', doc='Center data with mean'): True}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the pipeline to training documents.\n",
    "paramMap = {\n",
    "    binarizer.threshold: 20000, \n",
    "    imputer_mean.strategy: my_config['PIPELINE']['FIT']['imputer_strategy'], \n",
    "    standardScaler.withStd: my_config['PIPELINE']['FIT']['standardScaler_withStd'],\n",
    "    standardScaler.withMean: my_config['PIPELINE']['FIT']['standardScaler_withMean'],\n",
    "}\n",
    "paramMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_process_data = pipeline.fit(df_train_data, paramMap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{Param(parent='Binarizer_40d0dd5a4bbf', name='threshold', doc='threshold in binary classification prediction, in range [0, 1]'): 20000,\n",
       " Param(parent='Imputer_1c16f29b54b3', name='strategy', doc='strategy for imputation. If mean, then replace missing values using the mean value of the feature. If median, then replace missing values using the median value of the feature.'): 'mean',\n",
       " Param(parent='StandardScaler_2b12c1b1db3e', name='withStd', doc='Scale to unit standard deviation'): True,\n",
       " Param(parent='StandardScaler_2b12c1b1db3e', name='withMean', doc='Center data with mean'): False}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the pipeline to training documents.\n",
    "paramMap = {\n",
    "    binarizer.threshold: 20000, \n",
    "    imputer_mean.strategy: my_config['PIPELINE']['TRANSFORM']['imputer_strategy'], \n",
    "    standardScaler.withStd: my_config['PIPELINE']['TRANSFORM']['standardScaler_withStd'],\n",
    "    standardScaler.withMean: my_config['PIPELINE']['TRANSFORM']['standardScaler_withMean'],\n",
    "}\n",
    "paramMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_my = model_process_data.transform(df_train_data, paramMap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age_Days</th>\n",
       "      <th>Age_Days_binarized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13957.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14162.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16790.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23195.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11366.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>13881.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>21323.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>22493.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>16027.422948</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>20507.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Age_Days  Age_Days_binarized\n",
       "0  13957.000000                 0.0\n",
       "1  14162.000000                 0.0\n",
       "2  16790.000000                 0.0\n",
       "3  23195.000000                 1.0\n",
       "4  11366.000000                 0.0\n",
       "5  13881.000000                 0.0\n",
       "6  21323.000000                 1.0\n",
       "7  22493.000000                 1.0\n",
       "8  16027.422948                 0.0\n",
       "9  20507.000000                 1.0"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_my.limit(10).select(\"Age_Days\", \"Age_Days_binarized\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save date processing pipeline to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_process_data.write().overwrite().save(f\"{home}/data/my_data_processing_pipeline\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
